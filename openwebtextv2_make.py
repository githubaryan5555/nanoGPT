# STEP 1/4 , DATA COLLECTING.
import os
import requests
import pyarrow.parquet as pq

# =========================
# USER CONFIG
# =========================

DATASET = "Geralt-Targaryen/openwebtext2"

# how many shards you want
NUM_TOKENIZER_SHARDS = 3
NUM_MODEL_SHARDS     = 10

BASE_DIR = "data"
TOK_DIR  = os.path.join(BASE_DIR, "tokenizer_data")
MOD_DIR  = os.path.join(BASE_DIR, "model_data")

os.makedirs(TOK_DIR, exist_ok=True)
os.makedirs(MOD_DIR, exist_ok=True)

# =========================
# STEP 1: DETECT REAL SHARDS
# =========================

print("üîç Fetching dataset file list...")

api_url = f"https://huggingface.co/api/datasets/{DATASET}"
resp = requests.get(api_url)
resp.raise_for_status()

files = resp.json()["siblings"]

parquet_files = sorted(
    f["rfilename"]
    for f in files
    if f["rfilename"].endswith(".parquet")
)

TOTAL_SHARDS = len(parquet_files)

if TOTAL_SHARDS == 0:
    raise RuntimeError("‚ùå No parquet shards found. Dataset layout changed.")

print(f"‚úÖ Total parquet shards found: {TOTAL_SHARDS}")
print("First 5 shard names:")
for s in parquet_files[:5]:
    print(" ", s)
print("Last 5 shard names:")
for s in parquet_files[-5:]:
    print(" ", s)

# =========================
# STEP 2: DOWNLOAD FUNCTION
# =========================

def download_shards(shard_list, out_dir):
    for shard in shard_list:
        fname = os.path.basename(shard)
        url = f"https://huggingface.co/datasets/{DATASET}/resolve/main/{shard}"
        out = os.path.join(out_dir, fname)

        if os.path.exists(out):
            print(f"‚úî Already exists: {fname}")
            continue

        print(f"üì• Downloading {fname}")
        r = requests.get(url, stream=True)
        r.raise_for_status()

        with open(out, "wb") as f:
            for chunk in r.iter_content(1024 * 1024):
                if chunk:
                    f.write(chunk)

        size_mb = os.path.getsize(out) / (1024 * 1024)
        print(f"‚úÖ Saved {fname} ({size_mb:.2f} MB)")

# =========================
# STEP 3: SELECT SHARDS
# =========================

tokenizer_shards = parquet_files[:NUM_TOKENIZER_SHARDS]
model_shards     = parquet_files[:NUM_MODEL_SHARDS]

print(f"\nüß† Tokenizer shards: {len(tokenizer_shards)}")
print(f"ü§ñ Model shards: {len(model_shards)}")

# overlap is allowed and expected
download_shards(tokenizer_shards, TOK_DIR)
download_shards(model_shards, MOD_DIR)

# =========================
# STEP 4: INSPECT DATA
# =========================

def inspect_folder(folder, label):
    print(f"\n===== {label.upper()} INSPECTION =====")

    files = sorted(os.listdir(folder))
    all_text = []

    for f in files:
        path = os.path.join(folder, f)
        table = pq.read_table(path, columns=["text"])
        texts = table.column("text").to_pylist()
        all_text.extend(texts)

    print("\nüìå FIRST 100 SENTENCES:")
    for s in all_text[:100]:
        print(s.replace("\n", " ")[:300])

    print("\nüìå LAST 100 SENTENCES:")
    for s in all_text[-100:]:
        print(s.replace("\n", " ")[:300])

    print("\nüì¶ FILE SIZES:")
    for f in files:
        size_mb = os.path.getsize(os.path.join(folder, f)) / (1024 * 1024)
        print(f"{f}: {size_mb:.2f} MB")

inspect_folder(TOK_DIR, "tokenizer")
inspect_folder(MOD_DIR, "model")

print("\nüéâ DONE ‚Äî OpenWebText2 shards downloaded and verified.")
